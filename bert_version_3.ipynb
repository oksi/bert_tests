{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'trainSet.csv',\n",
    "    names=['search_term', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find max length to truncate/pad\n",
    "seq_len = [len(i.split()) for i in df.search_term]\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "max_len = 0\n",
    "for i in seq_len:\n",
    "    if i > max_len:\n",
    "        max_len = i\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twister picnic blanket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best stop smoking app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phosphorus fertiliser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tattoo books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>child's desk chair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              search_term\n",
       "0  twister picnic blanket\n",
       "1   best stop smoking app\n",
       "2   phosphorus fertiliser\n",
       "3            tattoo books\n",
       "4      child's desk chair"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_table('candidateTestSet.txt', names=['search_term'])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique existing categories\n",
    "possible_labels = df.category.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_LABELS = len(possible_labels) \n",
    "TOTAL_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.search_term.values,\n",
    "    df.category.values,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df.category.values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=12,                 \n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.search_term.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  used generators for sale uk\n",
      "Token IDs:  [101, 2109, 16937, 2005, 5096, 2866, 102, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[4]])[0].squeeze().numpy())\n",
    "print('Original: ', X[4])\n",
    "print('Token IDs: ', token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Run function `preprocessing_for_bert` on the train set, validation set, and test set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "test_inputs, test_masks = preprocessing_for_bert(df_test.search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Our Model\n",
    "### Create BertClassifier\n",
    "- create a BertClassifier class with a BERT model to extract the last hidden layer of the [CLS] token and a single-hidden-layer feed-forward neural network as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        b_hidden, cl_hidden, out = 768, 512, TOTAL_LABELS\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(b_hidden, cl_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cl_hidden, out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    \n",
    "                      eps=1e-8   \n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Train our Bert Classifier for 4 epochs. In each epoch, we will train our model and evaluate its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 1000 batches\n",
    "            if (step % 1000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 1000 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |  1000   |   6.866653   |     -      |     -     |  108.51  \n",
      "   1    |  2000   |   5.417363   |     -      |     -     |  105.58  \n",
      "   1    |  3000   |   4.280880   |     -      |     -     |  103.98  \n",
      "   1    |  4000   |   3.608044   |     -      |     -     |  104.03  \n",
      "   1    |  5000   |   3.194147   |     -      |     -     |  105.61  \n",
      "   1    |  6000   |   2.929214   |     -      |     -     |  105.95  \n",
      "   1    |  7000   |   2.773780   |     -      |     -     |  108.51  \n",
      "   1    |  8000   |   2.630089   |     -      |     -     |  110.26  \n",
      "   1    |  9000   |   2.534109   |     -      |     -     |  111.94  \n",
      "   1    |  10000  |   2.440166   |     -      |     -     |  108.78  \n",
      "   1    |  11000  |   2.352164   |     -      |     -     |  109.66  \n",
      "   1    |  12000  |   2.277778   |     -      |     -     |  109.32  \n",
      "   1    |  13000  |   2.264429   |     -      |     -     |  108.40  \n",
      "   1    |  14000  |   2.210559   |     -      |     -     |  110.94  \n",
      "   1    |  15000  |   2.170986   |     -      |     -     |  110.92  \n",
      "   1    |  16000  |   2.105013   |     -      |     -     |  110.91  \n",
      "   1    |  17000  |   2.075564   |     -      |     -     |  111.02  \n",
      "   1    |  17066  |   2.005698   |     -      |     -     |   7.33   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   3.062646   |  1.981451  |   57.66   |  1895.82 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |  1000   |   1.749471   |     -      |     -     |  107.99  \n",
      "   2    |  2000   |   1.745729   |     -      |     -     |  105.69  \n",
      "   2    |  3000   |   1.729853   |     -      |     -     |  104.84  \n",
      "   2    |  4000   |   1.717469   |     -      |     -     |  109.50  \n",
      "   2    |  5000   |   1.711943   |     -      |     -     |  105.80  \n",
      "   2    |  6000   |   1.680756   |     -      |     -     |  104.68  \n",
      "   2    |  7000   |   1.658222   |     -      |     -     |  105.63  \n",
      "   2    |  8000   |   1.640936   |     -      |     -     |  107.65  \n",
      "   2    |  9000   |   1.643611   |     -      |     -     |  106.09  \n",
      "   2    |  10000  |   1.652427   |     -      |     -     |  107.01  \n",
      "   2    |  11000  |   1.611495   |     -      |     -     |  105.91  \n",
      "   2    |  12000  |   1.619152   |     -      |     -     |  105.13  \n",
      "   2    |  13000  |   1.583533   |     -      |     -     |  104.40  \n",
      "   2    |  14000  |   1.589318   |     -      |     -     |  103.12  \n",
      "   2    |  15000  |   1.566322   |     -      |     -     |  104.48  \n",
      "   2    |  16000  |   1.567123   |     -      |     -     |  104.19  \n",
      "   2    |  17000  |   1.548760   |     -      |     -     |  103.71  \n",
      "   2    |  17066  |   1.508467   |     -      |     -     |   6.85   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   1.647473   |  1.709183  |   62.01   |  1844.76 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)  \n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 203 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.0.weight                                       (512, 768)\n",
      "classifier.0.bias                                             (512,)\n",
      "classifier.2.weight                                      (1419, 512)\n",
      "classifier.2.bias                                            (1419,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(bert_classifier.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Prediction Step\n",
    "The prediction step: perform a forward pass to compute logits and apply softmax to calculate probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the validation set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the entire Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |  1000   |   6.707998   |     -      |     -     |  113.53  \n",
      "   1    |  2000   |   5.153357   |     -      |     -     |  105.13  \n",
      "   1    |  3000   |   4.072509   |     -      |     -     |  105.76  \n",
      "   1    |  4000   |   3.470775   |     -      |     -     |  103.45  \n",
      "   1    |  5000   |   3.108213   |     -      |     -     |  104.56  \n",
      "   1    |  6000   |   2.866549   |     -      |     -     |  105.59  \n",
      "   1    |  7000   |   2.737394   |     -      |     -     |  104.11  \n",
      "   1    |  8000   |   2.592876   |     -      |     -     |  104.42  \n",
      "   1    |  9000   |   2.510552   |     -      |     -     |  105.76  \n",
      "   1    |  10000  |   2.412528   |     -      |     -     |  103.78  \n",
      "   1    |  11000  |   2.359293   |     -      |     -     |  104.17  \n",
      "   1    |  12000  |   2.277066   |     -      |     -     |  103.44  \n",
      "   1    |  13000  |   2.241460   |     -      |     -     |  104.29  \n",
      "   1    |  14000  |   2.200364   |     -      |     -     |  107.01  \n",
      "   1    |  15000  |   2.204249   |     -      |     -     |  104.58  \n",
      "   1    |  16000  |   2.144849   |     -      |     -     |  105.70  \n",
      "   1    |  17000  |   2.077351   |     -      |     -     |  103.98  \n",
      "   1    |  18000  |   2.046005   |     -      |     -     |  108.73  \n",
      "   1    |  18963  |   2.008241   |     -      |     -     |  105.02  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |  1000   |   1.730999   |     -      |     -     |  108.65  \n",
      "   2    |  2000   |   1.709083   |     -      |     -     |  110.80  \n",
      "   2    |  3000   |   1.710902   |     -      |     -     |  111.36  \n",
      "   2    |  4000   |   1.701899   |     -      |     -     |  111.40  \n",
      "   2    |  5000   |   1.683220   |     -      |     -     |  109.14  \n",
      "   2    |  6000   |   1.654471   |     -      |     -     |  111.40  \n",
      "   2    |  7000   |   1.655092   |     -      |     -     |  108.64  \n",
      "   2    |  8000   |   1.653054   |     -      |     -     |  103.03  \n",
      "   2    |  9000   |   1.640084   |     -      |     -     |  105.82  \n",
      "   2    |  10000  |   1.652756   |     -      |     -     |  104.12  \n",
      "   2    |  11000  |   1.617383   |     -      |     -     |  105.10  \n",
      "   2    |  12000  |   1.612196   |     -      |     -     |  105.61  \n",
      "   2    |  13000  |   1.610866   |     -      |     -     |  113.42  \n",
      "   2    |  14000  |   1.593101   |     -      |     -     |  112.04  \n",
      "   2    |  15000  |   1.571182   |     -      |     -     |  110.99  \n",
      "   2    |  16000  |   1.557844   |     -      |     -     |  112.93  \n",
      "   2    |  18000  |   1.558829   |     -      |     -     |  103.96  \n",
      "   2    |  18963  |   1.557507   |     -      |     -     |   98.84  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |  1000   |   1.221524   |     -      |     -     |  104.49  \n",
      "   3    |  3000   |   1.219120   |     -      |     -     |  104.11  \n",
      "   3    |  4000   |   1.211245   |     -      |     -     |  106.05  \n",
      "   3    |  5000   |   1.208511   |     -      |     -     |  106.86  \n",
      "   3    |  6000   |   1.204266   |     -      |     -     |  105.02  \n",
      "   3    |  7000   |   1.190236   |     -      |     -     |  104.70  \n",
      "   3    |  8000   |   1.207116   |     -      |     -     |  104.51  \n",
      "   3    |  9000   |   1.184558   |     -      |     -     |  104.17  \n",
      "   3    |  10000  |   1.191645   |     -      |     -     |  107.21  \n",
      "   3    |  11000  |   1.194178   |     -      |     -     |  109.10  \n",
      "   3    |  12000  |   1.193802   |     -      |     -     |  113.23  \n",
      "   3    |  13000  |   1.179714   |     -      |     -     |  113.96  \n",
      "   3    |  14000  |   1.187111   |     -      |     -     |  106.51  \n",
      "   3    |  15000  |   1.169823   |     -      |     -     |  105.63  \n",
      "   3    |  16000  |   1.156122   |     -      |     -     |  107.13  \n",
      "   3    |  17000  |   1.160310   |     -      |     -     |  105.78  \n",
      "   3    |  18000  |   1.154575   |     -      |     -     |  104.01  \n",
      "   3    |  18963  |   1.143375   |     -      |     -     |  101.45  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |  1000   |   0.917000   |     -      |     -     |  105.07  \n",
      "   4    |  2000   |   0.915048   |     -      |     -     |  103.14  \n",
      "   4    |  3000   |   0.893776   |     -      |     -     |  104.37  \n",
      "   4    |  4000   |   0.914397   |     -      |     -     |  106.78  \n",
      "   4    |  5000   |   0.899531   |     -      |     -     |  104.87  \n",
      "   4    |  6000   |   0.900113   |     -      |     -     |  105.01  \n",
      "   4    |  7000   |   0.911942   |     -      |     -     |  105.18  \n",
      "   4    |  8000   |   0.903596   |     -      |     -     |  106.47  \n",
      "   4    |  9000   |   0.901007   |     -      |     -     |  106.47  \n",
      "   4    |  10000  |   0.904023   |     -      |     -     |  104.87  \n",
      "   4    |  11000  |   0.899384   |     -      |     -     |  105.12  \n",
      "   4    |  12000  |   0.890128   |     -      |     -     |  104.37  \n",
      "   4    |  13000  |   0.891485   |     -      |     -     |  103.37  \n",
      "   4    |  14000  |   0.902881   |     -      |     -     |  104.52  \n",
      "   4    |  15000  |   0.902489   |     -      |     -     |  104.52  \n",
      "   4    |  16000  |   0.898758   |     -      |     -     |  105.92  \n",
      "   4    |  17000  |   0.892749   |     -      |     -     |  111.34  \n",
      "   4    |  18000  |   0.912849   |     -      |     -     |  104.75  \n",
      "   4    |  18963  |   0.900058   |     -      |     -     |  102.03  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the train set and the validation set\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
    "\n",
    "# Train the Bert Classifier on the entire training data\n",
    "set_seed(RANDOM_SEED)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
    "train(bert_classifier, full_train_dataloader, epochs=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
