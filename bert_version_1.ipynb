{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNTObU9gOGW5"
   },
   "source": [
    "### Explanatory data analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QvjYvLKoPmW9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast \n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-large-uncased'\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wLZgrxhwNmG9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/trainSet.csv',\n",
    "    names=['search_term', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmcMOaT3Nxhb",
    "outputId": "d5a7bb9e-0695-44b5-c4a8-724e0ce28615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.category.unique() # list of unique existing categories\n",
    "len(possible_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qOk3y-VhNxec"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_table('data/candidateTestSet.txt', names=['search_term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ_zPa7gQaxW"
   },
   "source": [
    "### Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xClTwGaMNxkJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SaK7OZmCOEW_"
   },
   "outputs": [],
   "source": [
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    df['search_term'],\n",
    "    df['category'],\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df['category'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7ssSyhanK3f",
    "outputId": "5d19cd1d-ea2e-4eba-f6e2-828defba456f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546140"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkASrOPqnW-g",
    "outputId": "c89b9f6e-7da5-4d14-c3ec-a5de586bc2c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60683"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTFPXERiTzbR"
   },
   "source": [
    "### Import BERT Model and BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "940y1l7CT6SU"
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nY7xUlclT6oL"
   },
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 12, # max length of search terms in train and test sets\n",
    "    padding=True,\n",
    "    truncation=True    \n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 12,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set - df_test\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    df_test['search_term'].tolist(),\n",
    "    max_length = 12,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OXx5URAaT6y5"
   },
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaI3XWMNavzx"
   },
   "source": [
    "### Creating Data Loaders\n",
    "Now we will create dataloaders for both train and validation set. These dataloaders will pass batches of train data and validation data as input to the model during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N_ce9yFqbaBE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gP9dksLfT69I"
   },
   "outputs": [],
   "source": [
    "#define a batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, \n",
    "                           train_mask, train_y)\n",
    "val_data = TensorDataset(val_seq, \n",
    "                         val_mask, val_y)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, \n",
    "    sampler=RandomSampler(train_data), \n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(\n",
    "    val_data, \n",
    "    sampler = SequentialSampler(val_data), \n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIXKxNDZc2OL"
   },
   "source": [
    "### Define Model Architecture\n",
    "Freeze the entire architecture\n",
    "\n",
    "Freeze all the layers of the model and attach a few neural network layers of our own and train this new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JzV24vNTdFRv"
   },
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AzDhkTUmdFV3"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT_Arch, self).__init__()\n",
    "      \n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, len(possible_labels))\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)[:2]\n",
    "      \n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qQDhOVY7dFdv"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udb7EcwoSdka",
    "outputId": "d3a743da-83db-4eb0-deaa-c9516e047dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 395 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 1024)\n",
      "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
      "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
      "bert.embeddings.LayerNorm.weight                             (1024,)\n",
      "bert.embeddings.LayerNorm.bias                               (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
      "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
      "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
      "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
      "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "fc1.weight                                               (512, 1024)\n",
      "fc1.bias                                                      (512,)\n",
      "fc2.weight                                               (1419, 512)\n",
      "fc2.bias                                                     (1419,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-51sDwYlds1e"
   },
   "source": [
    "### Setting Up Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lWf4Z1VkdjNx"
   },
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-5)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QEt5epAzhhxk"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps = total_steps \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07xR7GPfeR7J"
   },
   "source": [
    "There is a class imbalance in our dataset. So, we will first compute class weights for the labels in the train set and then pass these weights to the loss function so that it takes care of the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29sD3poidjdf",
    "outputId": "5e776e89-c1f0-4f17-8f51-2afb64604527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.94332518 1.0573535  0.73449747 ... 0.85338509 0.82063257 0.94332518]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9oo-97Ryfhv5"
   },
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights, dtype=torch.float)\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42w9Fzc2f8i3"
   },
   "source": [
    "### Fine-Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "V1SOsn1afh7d"
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 5000 batches.\n",
    "        if step % 5000 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vyX5udq3fiFn"
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 1000 batches.\n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoLKpRU7jPEy"
   },
   "source": [
    "#### Start fine-tuning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--8mGrAKiph8",
    "outputId": "4a43d388-6a72-4864-f3b8-c4170c774142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.252\n",
      "Validation Loss: 7.235\n",
      "\n",
      " Epoch 2 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.226\n",
      "Validation Loss: 7.204\n",
      "\n",
      " Epoch 3 / 100\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.199\n",
      "Validation Loss: 7.173\n",
      "\n",
      " Epoch 4 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.170\n",
      "Validation Loss: 7.139\n",
      "\n",
      " Epoch 5 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.137\n",
      "Validation Loss: 7.105\n",
      "\n",
      " Epoch 6 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.095\n",
      "Validation Loss: 7.060\n",
      "\n",
      " Epoch 7 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 7.047\n",
      "Validation Loss: 7.011\n",
      "\n",
      " Epoch 8 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.994\n",
      "Validation Loss: 6.955\n",
      "\n",
      " Epoch 9 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.937\n",
      "Validation Loss: 6.899\n",
      "\n",
      " Epoch 10 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.881\n",
      "Validation Loss: 6.839\n",
      "\n",
      " Epoch 11 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.827\n",
      "Validation Loss: 6.787\n",
      "\n",
      " Epoch 12 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.776\n",
      "Validation Loss: 6.728\n",
      "\n",
      " Epoch 13 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.727\n",
      "Validation Loss: 6.682\n",
      "\n",
      " Epoch 14 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.680\n",
      "Validation Loss: 6.634\n",
      "\n",
      " Epoch 15 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.636\n",
      "Validation Loss: 6.590\n",
      "\n",
      " Epoch 16 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.594\n",
      "Validation Loss: 6.548\n",
      "\n",
      " Epoch 17 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.555\n",
      "Validation Loss: 6.501\n",
      "\n",
      " Epoch 18 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.519\n",
      "Validation Loss: 6.467\n",
      "\n",
      " Epoch 19 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.482\n",
      "Validation Loss: 6.424\n",
      "\n",
      " Epoch 20 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.450\n",
      "Validation Loss: 6.393\n",
      "\n",
      " Epoch 21 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.417\n",
      "Validation Loss: 6.356\n",
      "\n",
      " Epoch 22 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.388\n",
      "Validation Loss: 6.324\n",
      "\n",
      " Epoch 23 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.360\n",
      "Validation Loss: 6.288\n",
      "\n",
      " Epoch 24 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.334\n",
      "Validation Loss: 6.257\n",
      "\n",
      " Epoch 25 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.306\n",
      "Validation Loss: 6.233\n",
      "\n",
      " Epoch 26 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.283\n",
      "Validation Loss: 6.200\n",
      "\n",
      " Epoch 27 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.259\n",
      "Validation Loss: 6.178\n",
      "\n",
      " Epoch 28 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.215\n",
      "Validation Loss: 6.129\n",
      "\n",
      " Epoch 30 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.195\n",
      "Validation Loss: 6.110\n",
      "\n",
      " Epoch 31 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Training Loss: 6.175\n",
      "Validation Loss: 6.083\n",
      "\n",
      " Epoch 32 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.157\n",
      "Validation Loss: 6.062\n",
      "\n",
      " Epoch 33 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.142\n",
      "Validation Loss: 6.048\n",
      "\n",
      " Epoch 34 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.122\n",
      "Validation Loss: 6.025\n",
      "\n",
      " Epoch 35 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.106\n",
      "Validation Loss: 6.011\n",
      "\n",
      " Epoch 36 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.093\n",
      "Validation Loss: 5.995\n",
      "\n",
      " Epoch 37 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.077\n",
      "Validation Loss: 5.973\n",
      "\n",
      " Epoch 38 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.065\n",
      "Validation Loss: 5.954\n",
      "\n",
      " Epoch 39 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.050\n",
      "Validation Loss: 5.944\n",
      "\n",
      " Epoch 40 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.039\n",
      "Validation Loss: 5.925\n",
      "\n",
      " Epoch 41 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.027\n",
      "Validation Loss: 5.913\n",
      "\n",
      " Epoch 42 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.015\n",
      "Validation Loss: 5.898\n",
      "\n",
      " Epoch 43 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 6.001\n",
      "Validation Loss: 5.883\n",
      "\n",
      " Epoch 44 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.988\n",
      "Validation Loss: 5.870\n",
      "\n",
      " Epoch 45 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.982\n",
      "Validation Loss: 5.859\n",
      "\n",
      " Epoch 46 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.969\n",
      "Validation Loss: 5.847\n",
      "\n",
      " Epoch 47 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.960\n",
      "Validation Loss: 5.836\n",
      "\n",
      " Epoch 48 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.950\n",
      "Validation Loss: 5.826\n",
      "\n",
      " Epoch 49 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.940\n",
      "Validation Loss: 5.817\n",
      "\n",
      " Epoch 50 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.932\n",
      "Validation Loss: 5.799\n",
      "\n",
      " Epoch 51 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.922\n",
      "Validation Loss: 5.797\n",
      "\n",
      " Epoch 52 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.915\n",
      "Validation Loss: 5.784\n",
      "\n",
      " Epoch 53 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.910\n",
      "Validation Loss: 5.774\n",
      "\n",
      " Epoch 54 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.901\n",
      "Validation Loss: 5.764\n",
      "\n",
      " Epoch 55 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.892\n",
      "Validation Loss: 5.755\n",
      "\n",
      " Epoch 56 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.884\n",
      "Validation Loss: 5.751\n",
      "\n",
      " Epoch 57 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.877\n",
      "Validation Loss: 5.742\n",
      "\n",
      " Epoch 58 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.870\n",
      "Validation Loss: 5.735\n",
      "\n",
      " Epoch 59 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.862\n",
      "Validation Loss: 5.725\n",
      "\n",
      " Epoch 60 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.856\n",
      "Validation Loss: 5.713\n",
      "\n",
      " Epoch 61 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.849\n",
      "Validation Loss: 5.711\n",
      "\n",
      " Epoch 62 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.846\n",
      "Validation Loss: 5.707\n",
      "\n",
      " Epoch 63 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.839\n",
      "Validation Loss: 5.699\n",
      "\n",
      " Epoch 64 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.832\n",
      "Validation Loss: 5.694\n",
      "\n",
      " Epoch 65 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.828\n",
      "Validation Loss: 5.687\n",
      "\n",
      " Epoch 66 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.823\n",
      "Validation Loss: 5.684\n",
      "\n",
      " Epoch 67 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.820\n",
      "Validation Loss: 5.678\n",
      "\n",
      " Epoch 68 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.813\n",
      "Validation Loss: 5.668\n",
      "\n",
      " Epoch 69 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.811\n",
      "Validation Loss: 5.667\n",
      "\n",
      " Epoch 70 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.808\n",
      "Validation Loss: 5.659\n",
      "\n",
      " Epoch 71 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.801\n",
      "Validation Loss: 5.653\n",
      "\n",
      " Epoch 72 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.798\n",
      "Validation Loss: 5.649\n",
      "\n",
      " Epoch 73 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.794\n",
      "Validation Loss: 5.643\n",
      "\n",
      " Epoch 74 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.787\n",
      "Validation Loss: 5.637\n",
      "\n",
      " Epoch 75 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.787\n",
      "Validation Loss: 5.635\n",
      "\n",
      " Epoch 76 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.783\n",
      "Validation Loss: 5.631\n",
      "\n",
      " Epoch 77 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.779\n",
      "Validation Loss: 5.629\n",
      "\n",
      " Epoch 78 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.776\n",
      "Validation Loss: 5.626\n",
      "\n",
      " Epoch 79 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.775\n",
      "Validation Loss: 5.620\n",
      "\n",
      " Epoch 80 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.770\n",
      "Validation Loss: 5.617\n",
      "\n",
      " Epoch 81 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.767\n",
      "Validation Loss: 5.613\n",
      "\n",
      " Epoch 82 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.766\n",
      "Validation Loss: 5.612\n",
      "\n",
      " Epoch 83 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.762\n",
      "Validation Loss: 5.609\n",
      "\n",
      " Epoch 84 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.760\n",
      "Validation Loss: 5.607\n",
      "\n",
      " Epoch 85 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.760\n",
      "Validation Loss: 5.602\n",
      "\n",
      " Epoch 86 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.758\n",
      "Validation Loss: 5.603\n",
      "\n",
      " Epoch 87 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.755\n",
      "Validation Loss: 5.600\n",
      "\n",
      " Epoch 88 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.752\n",
      "Validation Loss: 5.598\n",
      "\n",
      " Epoch 89 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.750\n",
      "Validation Loss: 5.595\n",
      "\n",
      " Epoch 90 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.750\n",
      "Validation Loss: 5.592\n",
      "\n",
      " Epoch 91 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.747\n",
      "Validation Loss: 5.591\n",
      "\n",
      " Epoch 92 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.748\n",
      "Validation Loss: 5.591\n",
      "\n",
      " Epoch 93 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.745\n",
      "Validation Loss: 5.590\n",
      "\n",
      " Epoch 94 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.742\n",
      "Validation Loss: 5.588\n",
      "\n",
      " Epoch 95 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.745\n",
      "Validation Loss: 5.588\n",
      "\n",
      " Epoch 96 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.742\n",
      "Validation Loss: 5.587\n",
      "\n",
      " Epoch 97 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.744\n",
      "Validation Loss: 5.587\n",
      "\n",
      " Epoch 98 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.743\n",
      "Validation Loss: 5.586\n",
      "\n",
      " Epoch 99 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "  Batch 15,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.743\n",
      "Validation Loss: 5.586\n",
      "\n",
      " Epoch 100 / 100\n",
      "  Batch 5,000  of  17,067.\n",
      "  Batch 10,000  of  17,067.\n",
      "\n",
      "Evaluating...\n",
      "  Batch 1,000  of  1,897.\n",
      "\n",
      "Training Loss: 5.741\n",
      "Validation Loss: 5.586\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "history = {'train': {}, 'val': {}}\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, EPOCHS))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'model/bert_version_1_saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    history['train'][epoch] = train_loss\n",
    "    history['val'][epoch] =  valid_loss\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "multiclass_classification_adthena.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
